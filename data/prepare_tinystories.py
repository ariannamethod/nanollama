"""
Download and prepare TinyStories dataset for quick smoke tests.
Better than Shakespeare for testing language modeling on modern data.

TinyStories is a small dataset of simple stories generated by GPT-3.5/4,
designed for training small language models. Fast to download, good quality.

Usage:
    python -m data.prepare_tinystories
"""

import os
import json
import argparse
import numpy as np
from typing import Optional

from nanollama.common import print0, get_base_dir

# TinyStories from HuggingFace datasets
TINYSTORIES_URL = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt"
TINYSTORIES_VAL_URL = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt"


def download_file(url: str, output_path: str) -> str:
    """Download a file if it doesn't exist."""
    if os.path.exists(output_path):
        print0(f"File already exists: {output_path}")
        return output_path
    
    print0(f"Downloading {url}...")
    import urllib.request
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    urllib.request.urlretrieve(url, output_path)
    print0(f"Downloaded to {output_path}")
    return output_path


def prepare_tinystories(
    output_dir: Optional[str] = None,
    max_stories: int = 10000,  # 10K stories â‰ˆ 5MB text, trains in ~1 min
    tokenize: bool = True,
    vocab_size: int = 4096,  # Small vocab for testing
):
    """
    Download TinyStories and prepare for training.
    
    Args:
        output_dir: Where to save prepared data
        max_stories: Maximum number of stories to use (for quick tests)
        tokenize: Whether to also create tokenized .bin files
        vocab_size: Vocabulary size for tokenizer (small for testing)
    """
    if output_dir is None:
        output_dir = os.path.join(get_base_dir(), "data", "tinystories")
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Download train file
    train_txt_path = os.path.join(output_dir, "train_raw.txt")
    download_file(TINYSTORIES_URL, train_txt_path)
    
    # Read and process stories
    print0("Processing stories...")
    with open(train_txt_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Stories are separated by <|endoftext|>
    stories = content.split("<|endoftext|>")
    stories = [s.strip() for s in stories if s.strip()]
    
    # Limit for quick testing
    if max_stories and len(stories) > max_stories:
        stories = stories[:max_stories]
    
    print0(f"Using {len(stories):,} stories")
    
    # Calculate total characters
    total_chars = sum(len(s) for s in stories)
    print0(f"Total: {total_chars:,} characters")
    
    # Split into train/val (95/5)
    n_val = max(1, len(stories) // 20)
    train_stories = stories[:-n_val]
    val_stories = stories[-n_val:]
    
    # Save as text files
    train_text = "\n\n".join(train_stories)
    val_text = "\n\n".join(val_stories)
    
    train_path = os.path.join(output_dir, "train.txt")
    val_path = os.path.join(output_dir, "val.txt")
    
    with open(train_path, 'w', encoding='utf-8') as f:
        f.write(train_text)
    
    with open(val_path, 'w', encoding='utf-8') as f:
        f.write(val_text)
    
    print0(f"Train: {len(train_stories):,} stories, {len(train_text):,} chars -> {train_path}")
    print0(f"Val: {len(val_stories):,} stories, {len(val_text):,} chars -> {val_path}")
    
    # Create tokenized version for training
    if tokenize:
        print0("\nTokenizing for training...")
        
        try:
            import sentencepiece as spm
        except ImportError:
            print0("sentencepiece not installed, skipping tokenization")
            print0("Install with: pip install sentencepiece")
            return output_dir
        
        # Train a small tokenizer on this data
        model_prefix = os.path.join(output_dir, "tokenizer")
        
        if not os.path.exists(f"{model_prefix}.model"):
            print0(f"Training tokenizer with vocab_size={vocab_size}...")
            spm.SentencePieceTrainer.train(
                input=train_path,
                model_prefix=model_prefix,
                vocab_size=vocab_size,
                model_type='bpe',
                character_coverage=0.9995,
                num_threads=4,
                split_digits=True,
                allow_whitespace_only_pieces=True,
                byte_fallback=True,
                unk_surface=" \u2047 ",
                normalization_rule_name="identity",
            )
            print0(f"Tokenizer saved to {model_prefix}.model")
        
        # Load tokenizer
        sp = spm.SentencePieceProcessor()
        sp.load(f"{model_prefix}.model")
        
        # Tokenize and save as binary
        for split, text in [("train", train_text), ("val", val_text)]:
            tokens = sp.encode(text)
            tokens = np.array(tokens, dtype=np.uint16)
            
            bin_path = os.path.join(output_dir, f"{split}.bin")
            tokens.tofile(bin_path)
            print0(f"{split}: {len(tokens):,} tokens -> {bin_path}")
    
    print0("\nDone!")
    print0(f"\nTo run smoke test:")
    print0(f"  python -m tests.smoke_test --data-dir={output_dir}")
    
    return output_dir


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Prepare TinyStories dataset")
    parser.add_argument("--output-dir", type=str, default=None, help="Output directory")
    parser.add_argument("--max-stories", type=int, default=10000, help="Max stories to use")
    parser.add_argument("--no-tokenize", action="store_true", help="Skip tokenization")
    parser.add_argument("--vocab-size", type=int, default=4096, help="Tokenizer vocab size")
    args = parser.parse_args()
    
    prepare_tinystories(
        output_dir=args.output_dir,
        max_stories=args.max_stories,
        tokenize=not args.no_tokenize,
        vocab_size=args.vocab_size,
    )
